%% -*- erlang -*-

%% @doc The tcp port dalmatiner_db listens on for the tcp API
{mapping, "tcp_port", "dalmatiner_db.tcp_port",
 [{default, {{tcp_port}} },
  {datatype, integer}]}.

%% @doc Number of acceptor processes to keep around for tcp connections.
{mapping, "tcp_listeners", "dalmatiner_db.tcp_listeners",
 [{default, 100},
  {datatype, integer}]}.

%% @doc Number of allowed concurrent tcp connections.
{mapping, "tcp_max_connections", "dalmatiner_db.tcp_max_connections",
 [{default, 1024},
  {datatype, integer}]}.

%% @doc The number of Asyncronous worker for a vnode.
{mapping, "async_workers", "metric_vnode.async_workers",
 [{datatype, integer},
  {default, 5}]}.

%% @doc When handling handoffs how much should be the largest chunk be.
%% Large chunks improve thourhgput but too large chunks will cause
%% timeouts.
{mapping, "handoff_max_chunk_size", "metric_vnode.handoff_chunk",
 [{datatype, bytesize},
  {default, "1KB"}]}.

%% @doc The datapoints stored per file, default equals 1 week of data
%% in a 1 second resolution.
{mapping, "points_per_file", "metric_vnode.points_per_file",
 [{datatype, integer},
  {default, 604800}]}.

%% @doc How many datapoints are cached before being written to disk. This
%% is a maximum value, writes also occour before a specific metric is read,
%% this effects lost data on crashes.
{mapping, "cache_points", "metric_vnode.cache_points",
 [{datatype, integer},
  {default, 120}]}.

%% @doc How long data is kept by default, measured either in points
%% or infinity for not deleting old data. This can be changed on a
%% per bucket level using ddb-admin
{mapping, "lifetime", "metric_vnode.lifetime",
 [{datatype, [integer, {atom, infinity}]},
  {default, "infinity"}]}.

%% @doc Maximum number of files kept open per mstore, settings
%% over two have no effect, settings below 0 eqal to 0.
{mapping, "mstore.max_files", "metric_vnode.max_files",
 [{default, 2},
  {datatype, integer}]}.

%% @doc How often a pertition will be picked to be vacuumed.
{mapping, "vacuum_interval", "dalmatiner_vacuum.interval",
 [{default, "1h"},
  {datatype, {duration, ms}}]}.

%% @doc The path data gets stored into.
{mapping, "platform_data_dir", "riak_core.platform_data_dir",
 [{default, "{{platform_data_dir}}"},
  {datatype, string}]}.

%% @doc The path data gets stored into.
{mapping, "run_user_home", "setup.home",
 [{default, "{{run_user_home}}"},
  hidden,
  {datatype, string}]}.

%% @doc Default ring creation size.  Make sure it is a power of 2,
%% e.g. 16, 32, 64, 128, 256, 512 etc
%% This is set to a default of 4 to work in a one server installation
%% if you plan to expand your system please set it higher and reads
%% the manual at http://bit.ly/1ciKRkS
{mapping, "ring_size", "riak_core.ring_creation_size",
 [{datatype, integer},
  {default, 64},
  {commented, 64},
  {validators, ["ring_size"]}
 ]}.

{validator, "ring_size", "not a power of 2 greater than 1",
 fun(Size) ->
         Size > 1 andalso (Size band (Size-1) =:= 0)
 end}.

%% @doc The number of copies of the data that is keeped.
%% For good consistency N > (R + W) should be true.
{mapping, "n", "dalmatiner_db.n",
 [{datatype, integer},
  {default, 1}]}.

%% @doc The number of replies requried for a read request to succeed
%% The lower the value the faster the replies but the more likely stale
%% data is returend.
{mapping, "r", "dalmatiner_db.r",
 [{datatype, integer},
  {default, 1}]}.

%% @doc The number of replies requried for a write request to succeed
%% The lower the value the faster the write but the more likely to only
%% write partial data.
{mapping, "w", "dalmatiner_db.w",
 [{datatype, integer},
  {default, 1}]}.

%% @doc Schema directory
{mapping, "schema_dir", "riak_core.schema_dirs",
 [{default, "./share/schema"},
  {datatype, string}]
}.

{translation,
 "riak_core.schema_dirs",
 fun(Conf) ->
         D = cuttlefish:conf_get("schema_dir", Conf),
         [D]
 end
}.

%% @doc handoff.ip is the network address that Riak binds to for
%% intra-cluster data handoff.
{mapping, "handoff.ip", "riak_core.handoff_ip", 
 [{default, "127.0.0.1" },
  {datatype, string},
  {validators, ["valid_ipaddr"]}]
}.

%% @doc enable / disable self monitoring
{mapping, "self_monitor", "dalmatiner_db.self_monitor",
 [{default, on},
  {datatype, flag}]
}.
%% @doc Transport compression for sending data between the vnode
%% and the reading node.
{mapping, "metrics.transport_compression",
 "dalmatiner_db.metric_transport_compression",
 [{default, snappy},
  {datatype, {enum, [snappy, none]}}]}.

%% @doc Weather to exclude repairs for reads done over the most
%% recent period of time. This can significanty reduce the number
%% of reads repairs when queries are executed against very recent data.
{mapping, "read_repair.delay",
 "dalmatiner_db.read_repair_min_time",
 [{default, "1m"},
  {datatype, {duration, ms}}]}.


%% @doc Number of asyncronous IO requests that won't block the vnode.
%% A highter number reduces blockage but can lead to piling up of requests.
{mapping, "io.max_async", "metric_vnode.max_async_io",
 [{datatype, integer},
  {default, 20}]}.

%% @doc How long a syncronous IO request is allowed to take before
%% considering it crashed.
{mapping, "io.timeout", "metric_vnode.sync_io_timeout",
 [{default, "30s"},
  {datatype, {duration, ms}}]}.


%% @doc Async reads are performed out of band of writes, this way
%% larger reads can be performed without blocking other actions.
{mapping, "io.paralell_reads", "metric_vnode.async_read",
 [{default, off},
  {datatype, flag}]}.

%% @doc Minimal size of read requests to be considered for asyncronous
%% reads, this has no effect when async_reads is set to off
{mapping, "io.paralell_reads.min_size", "metric_vnode.async_min_size",
 [{datatype, integer},
  {default, 1000}]}.

%% @doc The size of the async worker pool, more workers mean more possible
%% paralellism
{mapping, "io.paralell_reads.min_size", "metric_vnode.io_queue_size_size",
 [{datatype, integer},
  {default, 5}]}.


%% @doc Some requests to the vnodes are handled by an asyncronous worker pool.
%% This parameter allows for tuning this pools behaviour when it comes dealing
%% with requests that are queued.
%% The default (fifo) will serve requests in the order they arrive at the worker
%% pool. The alternative is to serve the requests in the reverse order, dealing
%% with the most recent request first.
%% There are pro's and con's for both aproaches, it is best to test out what
%% works best for the desired characteristics.
%%
%% As a very rought rule of thumb:
%%  - fifo will lead to lower extremes
%%  - filo will lead to lower medians/mediums
{mapping, "io.paralell_reads.queue_strategy", "metric_vnode.queue_strategy",
 [{default, fifo},
  {datatype, {enum, [fifo, filo]}}]}.
